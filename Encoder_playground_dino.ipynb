{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# DINOv2"
      ],
      "metadata": {
        "id": "BNURq-sdvCoW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Load DINOv2 onto GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else \"cpu\")\n",
        "model = torch.hub.load('facebookresearch/dinov2', 'dinov2_vits14').to(device)\n",
        "model.eval()\n",
        "\n",
        "# Transform for DINOv2 (zero-padding + normalization)\n",
        "dinov2_transform = transforms.Compose([\n",
        "    transforms.Pad((96, 96)),  # (224-32)/2 = 96 pixels padding\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# Transform for original CIFAR-10 (just ToTensor to get raw pixels)\n",
        "original_transform = transforms.ToTensor()\n",
        "\n",
        "# Load dataset twice (once for DINOv2, once for original)\n",
        "cifar_dinov2 = datasets.CIFAR10(\n",
        "    root='./data',\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=dinov2_transform,\n",
        ")\n",
        "\n",
        "cifar_original = datasets.CIFAR10(\n",
        "    root='./data',\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=original_transform,\n",
        ")\n",
        "\n",
        "# Create DataLoaders\n",
        "loader_dinov2 = torch.utils.data.DataLoader(\n",
        "    cifar_dinov2,\n",
        "    batch_size=512,\n",
        "    num_workers=4,\n",
        "    pin_memory=True,\n",
        ")\n",
        "\n",
        "loader_original = torch.utils.data.DataLoader(\n",
        "    cifar_original,\n",
        "    batch_size=512,\n",
        "    num_workers=4,\n",
        "    pin_memory=True,\n",
        ")\n",
        "\n",
        "# Extract DINOv2 embeddings\n",
        "embeddings, labels = [], []\n",
        "with torch.no_grad():\n",
        "    for images, targets in loader_dinov2:\n",
        "        images = images.to(device, non_blocking=True)\n",
        "        embeddings.append(model(images).cpu())\n",
        "        labels.append(targets)\n",
        "\n",
        "embeddings = torch.cat(embeddings)  # Shape: [10000, 384]\n",
        "labels = torch.cat(labels)  # Shape: [10000]\n",
        "\n",
        "# Extract original images (32x32, no padding/normalization)\n",
        "original_images = []\n",
        "for images, _ in loader_original:\n",
        "    original_images.append(images)\n",
        "\n",
        "original_images = torch.cat(original_images)  # Shape: [10000, 3, 32, 32]\n",
        "\n",
        "# Save results (optional)\n",
        "torch.save({\n",
        "    'embeddings': embeddings,\n",
        "    'labels': labels,\n",
        "    'original_images': original_images,\n",
        "}, 'cifar10_dinov2_features_and_originals.pt')\n",
        "\n",
        "print(\"Shapes:\")\n",
        "print(f\"Embeddings: {embeddings.shape}\")  # [10000, 384]\n",
        "print(f\"Labels: {labels.shape}\")  # [10000]\n",
        "print(f\"Original Images: {original_images.shape}\")  # [10000, 3, 32, 32]"
      ],
      "metadata": {
        "id": "loQojIFdP9ac",
        "outputId": "4bea027c-8fb8-49b6-d441-10ea52314488",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/facebookresearch_dinov2_main\n",
            "/root/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)\n",
            "  warnings.warn(\"xFormers is not available (SwiGLU)\")\n",
            "/root/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:33: UserWarning: xFormers is not available (Attention)\n",
            "  warnings.warn(\"xFormers is not available (Attention)\")\n",
            "/root/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:40: UserWarning: xFormers is not available (Block)\n",
            "  warnings.warn(\"xFormers is not available (Block)\")\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shapes:\n",
            "Embeddings: torch.Size([10000, 384])\n",
            "Labels: torch.Size([10000])\n",
            "Original Images: torch.Size([10000, 3, 32, 32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "def correlation_dissimilarity(emb1, emb2):\n",
        "    \"\"\"\n",
        "    emb1 (np.array) : embedding in one feature space\n",
        "    emb2 (np.array) : embedding in another feature space\n",
        "    \"\"\"\n",
        "    dissim1 = 1. - np.corrcoef(emb1)\n",
        "    dissim2 = 1. - np.corrcoef(emb2)\n",
        "\n",
        "    triu_indices = np.triu_indices_from(dissim1, k=1)\n",
        "    flat1 = dissim1[triu_indices]\n",
        "    flat2 = dissim2[triu_indices]\n",
        "\n",
        "    # Compute second-order similarity (Pearson correlation)\n",
        "    r, _ = pearsonr(flat1, flat2)\n",
        "    return r\n",
        "\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def train_linear_classifier(X, y, test_size=0.2, random_state=42, **kwargs):\n",
        "    \"\"\"\n",
        "    Trains a linear classifier (Logistic Regression) and returns the model and accuracy.\n",
        "\n",
        "    Parameters:\n",
        "    X (array-like): Feature matrix\n",
        "    y (array-like): Target vector\n",
        "    test_size (float): Proportion of data to use for testing (default: 0.2)\n",
        "    random_state (int): Random seed for reproducibility (default: 42)\n",
        "    **kwargs: Additional arguments to pass to LogisticRegression\n",
        "\n",
        "    Returns:\n",
        "    tuple: (trained_model, accuracy_score)\n",
        "    \"\"\"\n",
        "    # Split data into training and test sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=test_size, random_state=random_state\n",
        "    )\n",
        "\n",
        "    # Initialize and train the linear classifier\n",
        "    model = LogisticRegression(**kwargs)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions and calculate accuracy\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    return model, accuracy"
      ],
      "metadata": {
        "id": "6KahGlNZS9hN"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings_np = embeddings.detach().cpu().numpy()\n",
        "original_images_np = original_images.detach().cpu().numpy().reshape(original_images.shape[0], -1)\n",
        "# correlation_dissimilarity(embeddings_np[:1000], embeddings_np[1000:2000])\n",
        "correlation_dissimilarity(embeddings_np[:100], original_images_np[:100])"
      ],
      "metadata": {
        "id": "EhjVrZdVTdCI",
        "outputId": "3290eb84-bf93-4aff-f70b-bdb845f62a1b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.float64(0.23679215246431365)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ue13J-v3Tuoy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}