{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# DINOv2"
      ],
      "metadata": {
        "id": "BNURq-sdvCoW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Load DINOv2 onto GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else \"cpu\")\n",
        "model = torch.hub.load('facebookresearch/dinov2', 'dinov2_vits14').to(device)\n",
        "model.eval()\n",
        "\n",
        "# Transform for DINOv2 (zero-padding + normalization)\n",
        "dinov2_transform = transforms.Compose([\n",
        "    transforms.Pad((96, 96)),  # (224-32)/2 = 96 pixels padding\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# Transform for original CIFAR-10 (just ToTensor to get raw pixels)\n",
        "original_transform = transforms.ToTensor()\n",
        "\n",
        "# Load dataset twice (once for DINOv2, once for original)\n",
        "cifar_dinov2 = datasets.CIFAR10(\n",
        "    root='./data',\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=dinov2_transform,\n",
        ")\n",
        "\n",
        "cifar_original = datasets.CIFAR10(\n",
        "    root='./data',\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=original_transform,\n",
        ")\n",
        "\n",
        "# Create DataLoaders\n",
        "loader_dinov2 = torch.utils.data.DataLoader(\n",
        "    cifar_dinov2,\n",
        "    batch_size=512,\n",
        "    num_workers=4,\n",
        "    pin_memory=True,\n",
        ")\n",
        "\n",
        "loader_original = torch.utils.data.DataLoader(\n",
        "    cifar_original,\n",
        "    batch_size=512,\n",
        "    num_workers=4,\n",
        "    pin_memory=True,\n",
        ")\n",
        "\n",
        "# Extract DINOv2 embeddings\n",
        "embeddings, labels = [], []\n",
        "with torch.no_grad():\n",
        "    for images, targets in loader_dinov2:\n",
        "        images = images.to(device, non_blocking=True)\n",
        "        embeddings.append(model(images).cpu())\n",
        "        labels.append(targets)\n",
        "\n",
        "embeddings = torch.cat(embeddings)  # Shape: [10000, 384]\n",
        "labels = torch.cat(labels)  # Shape: [10000]\n",
        "\n",
        "# Extract original images (32x32, no padding/normalization)\n",
        "original_images = []\n",
        "for images, _ in loader_original:\n",
        "    original_images.append(images)\n",
        "\n",
        "original_images = torch.cat(original_images)  # Shape: [10000, 3, 32, 32]\n",
        "\n",
        "# Save results (optional)\n",
        "torch.save({\n",
        "    'embeddings': embeddings,\n",
        "    'labels': labels,\n",
        "    'original_images': original_images,\n",
        "}, 'cifar10_dinov2_features_and_originals.pt')\n",
        "\n",
        "print(\"Shapes:\")\n",
        "print(f\"Embeddings: {embeddings.shape}\")  # [10000, 384]\n",
        "print(f\"Labels: {labels.shape}\")  # [10000]\n",
        "print(f\"Original Images: {original_images.shape}\")  # [10000, 3, 32, 32]"
      ],
      "metadata": {
        "id": "loQojIFdP9ac",
        "outputId": "4bea027c-8fb8-49b6-d441-10ea52314488",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/facebookresearch_dinov2_main\n",
            "/root/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)\n",
            "  warnings.warn(\"xFormers is not available (SwiGLU)\")\n",
            "/root/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:33: UserWarning: xFormers is not available (Attention)\n",
            "  warnings.warn(\"xFormers is not available (Attention)\")\n",
            "/root/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:40: UserWarning: xFormers is not available (Block)\n",
            "  warnings.warn(\"xFormers is not available (Block)\")\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shapes:\n",
            "Embeddings: torch.Size([10000, 384])\n",
            "Labels: torch.Size([10000])\n",
            "Original Images: torch.Size([10000, 3, 32, 32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "def correlation_dissimilarity(emb1, emb2):\n",
        "    \"\"\"\n",
        "    emb1 (np.array) : embedding in one feature space\n",
        "    emb2 (np.array) : embedding in another feature space\n",
        "    \"\"\"\n",
        "    dissim1 = 1. - np.corrcoef(emb1)\n",
        "    dissim2 = 1. - np.corrcoef(emb2)\n",
        "\n",
        "    triu_indices = np.triu_indices_from(dissim1, k=1)\n",
        "    flat1 = dissim1[triu_indices]\n",
        "    flat2 = dissim2[triu_indices]\n",
        "\n",
        "    # Compute second-order similarity (Pearson correlation)\n",
        "    r, _ = pearsonr(flat1, flat2)\n",
        "    return r\n",
        "\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def train_linear_classifier(X, y, test_size=0.2, random_state=42, **kwargs):\n",
        "    \"\"\"\n",
        "    Trains a linear classifier (Logistic Regression) and returns the model and accuracy.\n",
        "\n",
        "    Parameters:\n",
        "    X (array-like): Feature matrix\n",
        "    y (array-like): Target vector\n",
        "    test_size (float): Proportion of data to use for testing (default: 0.2)\n",
        "    random_state (int): Random seed for reproducibility (default: 42)\n",
        "    **kwargs: Additional arguments to pass to LogisticRegression\n",
        "\n",
        "    Returns:\n",
        "    tuple: (trained_model, accuracy_score)\n",
        "    \"\"\"\n",
        "    # Split data into training and test sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=test_size, random_state=random_state\n",
        "    )\n",
        "\n",
        "    # Initialize and train the linear classifier\n",
        "    model = LogisticRegression(**kwargs)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions and calculate accuracy\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    return model, accuracy\n",
        "\n",
        "def encode_set(encoder_function: callable, loader, original_loader, device = \"cpu\"):\n",
        "    embeddings, labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for images, targets in loader:\n",
        "            images = images.to(device, non_blocking=True)\n",
        "            embeddings.append(encoder_function(images))\n",
        "            labels.append(targets)\n",
        "\n",
        "        embeddings = torch.cat(embeddings)  # Shape: [10000, 384]\n",
        "        labels = torch.cat(labels)  # Shape: [10000]\n",
        "\n",
        "        # Extract original images (32x32, no padding/normalization)\n",
        "        original_images = []\n",
        "        for images, _ in original_loader:\n",
        "            original_images.append(images)\n",
        "\n",
        "        original_images = torch.cat(original_images)\n",
        "\n",
        "    return (embeddings.detach().cpu().numpy(),\n",
        "            labels.detach().cpu().numpy(),\n",
        "            original_images.detach().cpu().numpy().reshape(original_images.shape[0], -1))\n",
        "\n",
        "def run(encoder_function: callable, loader: torch.utils.data.DataLoader, original_loader: torch.utils.data.DataLoader,\n",
        "        logger, device = \"cpu\"):\n",
        "    embeddings_np, labels_np, original_images_np = encode_set(encoder_function, loader, original_loader, device)\n",
        "    logger.log({\n",
        "        \"classification_accuracy\" : train_linear_classifier(embeddings_np, labels_np)[1],\n",
        "        \"second_order_similarity\" : correlation_dissimilarity(embeddings_np, original_images_np)\n",
        "    })\n"
      ],
      "metadata": {
        "id": "6KahGlNZS9hN"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings_np = embeddings.detach().cpu().numpy()\n",
        "original_images_np = original_images.detach().cpu().numpy().reshape(original_images.shape[0], -1)\n",
        "# correlation_dissimilarity(embeddings_np[:1000], embeddings_np[1000:2000])\n",
        "correlation_dissimilarity(embeddings_np[:100], original_images_np[:100])"
      ],
      "metadata": {
        "id": "EhjVrZdVTdCI",
        "outputId": "3290eb84-bf93-4aff-f70b-bdb845f62a1b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.float64(0.23679215246431365)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ue13J-v3Tuoy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DHsV8rxY9t5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LaqhS8k19t2F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "import timm  # For ViT models\n",
        "import copy\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "import wandb\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# --- DINO Loss ---\n",
        "class DINOLoss(nn.Module):\n",
        "    def __init__(self, out_dim, teacher_temp=0.04, student_temp=0.1, center_momentum=0.9):\n",
        "        super().__init__()\n",
        "        self.teacher_temp = teacher_temp\n",
        "        self.student_temp = student_temp\n",
        "        self.center_momentum = center_momentum\n",
        "        self.center = torch.zeros(1, out_dim).to(device)\n",
        "\n",
        "    def forward(self, student_out, teacher_out):\n",
        "        student_out = F.log_softmax(student_out / self.student_temp, dim=-1)\n",
        "        teacher_out = F.softmax((teacher_out - self.center) / self.teacher_temp, dim=-1)\n",
        "        loss = -torch.sum(teacher_out * student_out, dim=-1).mean()\n",
        "        # update center\n",
        "        self.center = self.center * self.center_momentum + (1 - self.center_momentum) * teacher_out.mean(dim=0, keepdim=True)\n",
        "        return loss\n",
        "\n",
        "# --- Data Augmentations ---\n",
        "def get_dino_transforms():\n",
        "    global_crop = transforms.Compose([\n",
        "        transforms.RandomResizedCrop(224, scale=(0.5, 1.0)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ColorJitter(0.4, 0.4, 0.4, 0.1),\n",
        "        transforms.RandomGrayscale(p=0.2),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "    ])\n",
        "    return global_crop\n",
        "\n",
        "class MultiCropWrapper(torch.utils.data.Dataset):\n",
        "    def __init__(self, base_dataset, transform, num_crops=2):\n",
        "        self.base_dataset = base_dataset\n",
        "        self.transform = transform\n",
        "        self.num_crops = num_crops\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.base_dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img, _ = self.base_dataset[idx]\n",
        "        crops = [self.transform(img) for _ in range(self.num_crops)]\n",
        "        return crops\n",
        "\n",
        "# --- Models ---\n",
        "def get_vit_model():\n",
        "    model = timm.create_model('vit_small_patch8_224', pretrained=False)\n",
        "    model.head = nn.Identity()\n",
        "    return model.to(device)\n",
        "\n",
        "# --- Training Setup ---\n",
        "def train_one_epoch(student, teacher, loss_fn, optimizer, dataloader, momentum=0.996):\n",
        "    student.train()\n",
        "    total_loss = 0\n",
        "    num_batches = 0\n",
        "    for crops in tqdm(dataloader):\n",
        "        crops = [c.to(device, non_blocking=True) for c in crops]\n",
        "        global_crops = crops[:2]\n",
        "\n",
        "        student_out = [student(crop.unsqueeze(0)) for crop in global_crops]\n",
        "        teacher.eval()\n",
        "        with torch.no_grad():\n",
        "            teacher_out = teacher(global_crops[0].unsqueeze(0))\n",
        "\n",
        "        loss = sum(loss_fn(s_out, teacher_out) for s_out in student_out) / len(student_out)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # EMA update teacher\n",
        "        with torch.no_grad():\n",
        "            for ps, pt in zip(student.parameters(), teacher.parameters()):\n",
        "                pt.data.mul_(momentum).add_((1. - momentum) * ps.data)\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        num_batches += 1\n",
        "    return total_loss/num_batches\n",
        "        # print(f\"Loss: {loss.item():.4f}\")\n",
        "\n",
        "# --- Putting It All Together ---\n",
        "def main():\n",
        "    transform = get_dino_transforms()\n",
        "    dataset = datasets.CIFAR10(root='./data', train=True, download=True)\n",
        "    dino_dataset = MultiCropWrapper(dataset, transform, num_crops=2)\n",
        "    dataloader = torch.utils.data.DataLoader(dino_dataset, batch_size=64, shuffle=True, num_workers=4, collate_fn=lambda x: list(zip(*x))[0])\n",
        "\n",
        "    run_name = f'finetuning_dino'\n",
        "    num_epochs = 2\n",
        "    config = {\n",
        "        \"encoder\" : \"vit_small_patch8_224\",\n",
        "        \"num_epochs\" : num_epochs\n",
        "    }\n",
        "\n",
        "    logger = wandb.init(project = 'CV_frameworks', config = config, name = run_name)\n",
        "\n",
        "    student = get_vit_model()\n",
        "    teacher = copy.deepcopy(student)\n",
        "    for p in teacher.parameters():\n",
        "        p.requires_grad = False\n",
        "\n",
        "    loss_fn = DINOLoss(out_dim=384)  # ViT-small has 384 dim\n",
        "    optimizer = torch.optim.AdamW(student.parameters(), lr=3e-4, weight_decay=0.1)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"Epoch {epoch+1}\")\n",
        "        epoch_loss = train_one_epoch(student, teacher, loss_fn, optimizer, dataloader)\n",
        "        logger.log({\n",
        "                    \"epoch_loss\": epoch_loss\n",
        "                    },\n",
        "                    step = epoch + 1\n",
        "        )\n",
        "\n",
        "    logger.finish()\n",
        "    torch.save(student.state_dict(), \"finetuned_dino_student.pth\")\n",
        "\n",
        "\n",
        "main()\n"
      ],
      "metadata": {
        "id": "NRfKUo_u9tzW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 534
        },
        "outputId": "e3b036ae-b070-43a3-c947-f6c9b543acaa"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250512_021933-kftdm10g</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/ms-hate-life-/CV_frameworks/runs/kftdm10g' target=\"_blank\">finetuning_dino</a></strong> to <a href='https://wandb.ai/ms-hate-life-/CV_frameworks' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/ms-hate-life-/CV_frameworks' target=\"_blank\">https://wandb.ai/ms-hate-life-/CV_frameworks</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/ms-hate-life-/CV_frameworks/runs/kftdm10g' target=\"_blank\">https://wandb.ai/ms-hate-life-/CV_frameworks/runs/kftdm10g</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 782/782 [10:35<00:00,  1.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 782/782 [10:34<00:00,  1.23it/s]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch_loss</td><td>█▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch_loss</td><td>1.43545</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">finetuning_dino</strong> at: <a href='https://wandb.ai/ms-hate-life-/CV_frameworks/runs/kftdm10g' target=\"_blank\">https://wandb.ai/ms-hate-life-/CV_frameworks/runs/kftdm10g</a><br> View project at: <a href='https://wandb.ai/ms-hate-life-/CV_frameworks' target=\"_blank\">https://wandb.ai/ms-hate-life-/CV_frameworks</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250512_021933-kftdm10g/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "student = get_vit_model().to(device)\n",
        "student.load_state_dict(torch.load(\"finetuned_dino_student.pth\"))\n",
        "student.eval()"
      ],
      "metadata": {
        "id": "V1xFphur9--n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dinov2_transform = transforms.Compose([\n",
        "    transforms.Pad((96, 96)),  # (224-32)/2 = 96 pixels padding\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# Transform for original CIFAR-10 (just ToTensor to get raw pixels)\n",
        "original_transform = transforms.ToTensor()\n",
        "\n",
        "# Load dataset twice (once for DINOv2, once for original)\n",
        "cifar_dinov2 = datasets.CIFAR10(\n",
        "    root='./data',\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=dinov2_transform,\n",
        ")\n",
        "\n",
        "cifar_original = datasets.CIFAR10(\n",
        "    root='./data',\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=original_transform,\n",
        ")\n",
        "\n",
        "# Create DataLoaders\n",
        "loader_dinov2 = torch.utils.data.DataLoader(\n",
        "    cifar_dinov2,\n",
        "    batch_size=512,\n",
        "    num_workers=4,\n",
        "    pin_memory=True,\n",
        ")\n",
        "\n",
        "loader_original = torch.utils.data.DataLoader(\n",
        "    cifar_original,\n",
        "    batch_size=512,\n",
        "    num_workers=4,\n",
        "    pin_memory=True,\n",
        ")\n",
        "\n",
        "embeddings, labels, original_images = encode_set(student, loader_dinov2, loader_original, device=device)"
      ],
      "metadata": {
        "id": "kLEbUi2qSfLS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "correlation_dissimilarity(embeddings, original_images)\n",
        "train_linear_classifier(embeddings, labels)"
      ],
      "metadata": {
        "id": "UObroWEnZNHf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}